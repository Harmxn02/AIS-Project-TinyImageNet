{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI & Security Project\n",
    "\n",
    "**regular_models.ipynb**: in this notebook we try out several image classification models on our dataset 'TinyImageNet'. We use an **extreme** amount of different configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. dataset and libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_directory = r\"./data/TinyImageNet/tiny-imagenet-200/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "def download_and_prepare_tinyimagenet(data_dir=\"tiny-imagenet\"):\n",
    "\t\"\"\"\n",
    "\tDownload and extract the TinyImageNet dataset, and prepare it for PyTorch's ImageFolder.\n",
    "\t\"\"\"\n",
    "\turl = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "\tzip_file = \"tiny-imagenet-200.zip\"\n",
    "\textract_dir = \"tiny-imagenet-200\"\n",
    "\t\n",
    "\t# Create the directory if it doesn't exist\n",
    "\tif not os.path.exists(data_dir):\n",
    "\t\tos.makedirs(data_dir)\n",
    "\t\n",
    "\t# Download the dataset\n",
    "\tprint(\"Downloading TinyImageNet dataset...\")\n",
    "\tresponse = requests.get(url, stream=True)\n",
    "\twith open(os.path.join(data_dir, zip_file), \"wb\") as f:\n",
    "\t\tshutil.copyfileobj(response.raw, f)\n",
    "\tprint(\"Download complete.\")\n",
    "\t\n",
    "\t# Extract the dataset\n",
    "\tprint(\"Extracting dataset...\")\n",
    "\twith zipfile.ZipFile(os.path.join(data_dir, zip_file), \"r\") as zip_ref:\n",
    "\t\tzip_ref.extractall(data_dir)\n",
    "\tprint(\"Extraction complete.\")\n",
    "\t\n",
    "\t# Organize validation dataset\n",
    "\tval_dir = os.path.join(data_dir, extract_dir, \"val\")\n",
    "\tval_images_dir = os.path.join(val_dir, \"images\")\n",
    "\tval_annotations = os.path.join(val_dir, \"val_annotations.txt\")\n",
    "\t\n",
    "\t# Parse the validation annotations\n",
    "\tprint(\"Organizing validation dataset...\")\n",
    "\twith open(val_annotations, \"r\") as f:\n",
    "\t\tlines = f.readlines()\n",
    "\t\n",
    "\tval_labels = {}\n",
    "\tfor line in lines:\n",
    "\t\tparts = line.split(\"\\t\")\n",
    "\t\tval_labels[parts[0]] = parts[1]\n",
    "\t\n",
    "\t# Create subdirectories for validation classes\n",
    "\tfor label in set(val_labels.values()):\n",
    "\t\tos.makedirs(os.path.join(val_dir, label), exist_ok=True)\n",
    "\t\n",
    "\t# Move validation images to corresponding class subdirectories\n",
    "\tfor img, label in val_labels.items():\n",
    "\t\tsrc_path = os.path.join(val_images_dir, img)\n",
    "\t\tdst_path = os.path.join(val_dir, label, img)\n",
    "\t\tshutil.move(src_path, dst_path)\n",
    "\t\n",
    "\t# Remove the original images directory and annotation file\n",
    "\tshutil.rmtree(val_images_dir)\n",
    "\tos.remove(val_annotations)\n",
    "\t\n",
    "\tprint(\"Dataset is ready at:\", os.path.join(data_dir, extract_dir))\n",
    "\treturn os.path.join(data_dir, extract_dir)\n",
    "\n",
    "\n",
    "# Check if the dataset is already downloaded, by checking is /train/, /val/ and /test/ directories are present\n",
    "downloaded = os.path.exists(data_directory + \"train/\")\\\n",
    "    and os.path.exists(data_directory + \"test/\")\\\n",
    "    and os.path.exists(data_directory + \"val/\")\n",
    "\n",
    "\n",
    "if not downloaded:\n",
    "\tdataset_path = download_and_prepare_tinyimagenet(data_directory)\n",
    "\tprint(\"TinyImageNet dataset is available at:\", dataset_path)\n",
    "\n",
    "# This took 3m30 on my machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet18_Weights, ResNet34_Weights, \\\n",
    "\t\t\t\t\t\t\t\tResNet50_Weights, ResNet101_Weights, \\\n",
    "\t\t\t\t\t\t\t\tResNet152_Weights\n",
    "\n",
    "\n",
    "# Model definitions\n",
    "model_definitions = {\n",
    "\t\"ResNet18\": lambda: models.resnet18(weights=ResNet18_Weights.DEFAULT),\n",
    "\t\"ResNet50\": lambda: models.resnet50(weights=ResNet50_Weights.DEFAULT),\n",
    "\t\"ResNet152\": lambda: models.resnet152(weights=ResNet152_Weights.DEFAULT),\n",
    "}\n",
    "\n",
    "# Model configurations (hyperparameters)\n",
    "model_configs = {\n",
    "    \n",
    "    # Configurations with learning rate 0.1\n",
    "    \"Config-1A-0.3WD\": {\"lr\": 0.1, \"optimizer\": \"SGD\", \"weight_decay\": 0.3},\n",
    "    \"Config-1A-0.9WD\": {\"lr\": 0.1, \"optimizer\": \"SGD\", \"weight_decay\": 0.9},\n",
    "    \"Config-1B-0.3WD\": {\"lr\": 0.1, \"optimizer\": \"AdamW\", \"weight_decay\": 0.3},\n",
    "    \"Config-1B-0.9WD\": {\"lr\": 0.1, \"optimizer\": \"AdamW\", \"weight_decay\": 0.9},\n",
    "    \n",
    "    # Configurations with learning rate 0.01\n",
    "    \"Config-2A-0.3WD\": {\"lr\": 0.01, \"optimizer\": \"SGD\", \"weight_decay\": 0.3},\n",
    "    \"Config-2A-0.9WD\": {\"lr\": 0.01, \"optimizer\": \"SGD\", \"weight_decay\": 0.9},\n",
    "    \"Config-2B-0.3WD\": {\"lr\": 0.01, \"optimizer\": \"AdamW\", \"weight_decay\": 0.3},\n",
    "    \"Config-2B-0.9WD\": {\"lr\": 0.01, \"optimizer\": \"AdamW\", \"weight_decay\": 0.9},\n",
    "    \n",
    "    # Configurations with learning rate 0.001\n",
    "    \"Config-3A-0.3WD\": {\"lr\": 0.001, \"optimizer\": \"SGD\", \"weight_decay\": 0.3},\n",
    "    \"Config-3A-0.9WD\": {\"lr\": 0.001, \"optimizer\": \"SGD\", \"weight_decay\": 0.9},\n",
    "    \"Config-3B-0.3WD\": {\"lr\": 0.001, \"optimizer\": \"AdamW\", \"weight_decay\": 0.3},\n",
    "    \"Config-3B-0.9WD\": {\"lr\": 0.001, \"optimizer\": \"AdamW\", \"weight_decay\": 0.9},\n",
    "    \n",
    "    # Configurations with learning rate 0.0001\n",
    "    \"Config-4A-0.3WD\": {\"lr\": 0.0001, \"optimizer\": \"SGD\", \"weight_decay\": 0.3},\n",
    "    \"Config-4A-0.9WD\": {\"lr\": 0.0001, \"optimizer\": \"SGD\", \"weight_decay\": 0.9},\n",
    "    \"Config-4B-0.3WD\": {\"lr\": 0.0001, \"optimizer\": \"AdamW\", \"weight_decay\": 0.3},\n",
    "    \"Config-4B-0.9WD\": {\"lr\": 0.0001, \"optimizer\": \"AdamW\", \"weight_decay\": 0.9},\n",
    "}\n",
    "\n",
    "# General hyperparameters\n",
    "hyperparameters = {\n",
    "\t'batch_size': 64,\n",
    "\t'epochs': 20,\n",
    "\t'num_workers': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: set-up training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define TinyImageNet Dataset\n",
    "def get_tinyimagenet_loaders(data_dir, batch_size, num_workers=hyperparameters['num_workers']):\n",
    "\t\"\"\"\n",
    "\tSet up data loaders for TinyImageNet dataset.\n",
    "\t\"\"\"\n",
    "\ttransform_train = transforms.Compose([\n",
    "\t\ttransforms.RandomCrop(64, padding=4),\n",
    "\t\ttransforms.RandomHorizontalFlip(),\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize(mean=[0.4802, 0.4481, 0.3975], std=[0.2302, 0.2265, 0.2262])\n",
    "\t])\n",
    "\n",
    "\ttransform_val = transforms.Compose([\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize(mean=[0.4802, 0.4481, 0.3975], std=[0.2302, 0.2265, 0.2262])\n",
    "\t])\n",
    "\n",
    "\ttrain_dataset = datasets.ImageFolder(root=f\"{data_dir}/train\", transform=transform_train)\n",
    "\tval_dataset = datasets.ImageFolder(root=f\"{data_dir}/val\", transform=transform_val)\n",
    "\n",
    "\ttrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\tval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "\treturn train_loader, val_loader\n",
    "\n",
    "# Step 2: Define Training and Evaluation Functions\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "\t\"\"\"\n",
    "\tTrain the model for one epoch.\n",
    "\t\"\"\"\n",
    "\tmodel.train()\n",
    "\trunning_loss = 0.0\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\n",
    "\tfor inputs, targets in tqdm(loader, desc=\"\tTraining\", leave=False):\n",
    "\t# for inputs, targets in loader:\n",
    "\t\tinputs, targets = inputs.to(device), targets.to(device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\toutputs = model(inputs)\n",
    "\t\tloss = criterion(outputs, targets)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\trunning_loss += loss.item() * inputs.size(0)\n",
    "\t\t_, predicted = outputs.max(1)\n",
    "\t\tcorrect += predicted.eq(targets).sum().item()\n",
    "\t\ttotal += targets.size(0)\n",
    "\n",
    "\tepoch_loss = running_loss / total\n",
    "\taccuracy = correct / total\n",
    "\treturn epoch_loss, accuracy\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "\t\"\"\"\n",
    "\tEvaluate the model on validation data and compute top-1 and top-5 errors.\n",
    "\t\"\"\"\n",
    "\tmodel.eval()\n",
    "\trunning_loss = 0.0\n",
    "\ttop1_correct = 0\n",
    "\ttotal = 0\n",
    "\ttop5_correct = 0\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, targets in tqdm(loader, desc=\"\tValidation\", leave=False):\n",
    "\t\t# for inputs, targets in loader:\n",
    "\t\t\tinputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "\t\t\toutputs = model(inputs)\n",
    "\t\t\tloss = criterion(outputs, targets)\n",
    "\t\t\trunning_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\t\t\t_, top1_preds = outputs.max(1)\n",
    "\t\t\ttop1_correct += top1_preds.eq(targets).sum().item()\n",
    "\n",
    "\t\t\t_, top5_preds = outputs.topk(5, 1, True, True)\n",
    "\t\t\ttop5_correct += sum([targets[i] in top5_preds[i] for i in range(targets.size(0))])\n",
    "\t\t\ttotal += targets.size(0)\n",
    "\n",
    "\tepoch_loss = running_loss / total\n",
    "\ttop1_error = 1 - (top1_correct / total)\n",
    "\ttop5_error = 1 - (top5_correct / total)\n",
    "\n",
    "\treturn epoch_loss, top1_error, top5_error\n",
    "\n",
    "# Step 3: Define Model Training and Comparison\n",
    "def compare_models(models, model_configs, hyperparameters, data_dir, num_epochs, device=\"cuda\"):\n",
    "\t\"\"\"\n",
    "\tFine-tune and evaluate multiple models with different configurations.\n",
    "\t\"\"\"\n",
    "\tresults = []\n",
    "\n",
    "\ttrain_loader, val_loader = get_tinyimagenet_loaders(data_dir, batch_size=hyperparameters['batch_size'])\n",
    "\n",
    "\tfor model_name, model_fn in models.items():\n",
    "\t\tfor config_name, config in model_configs.items():\n",
    "\t\t\tprint(f\"Training {model_name} with configuration {config_name}:\")\n",
    "\t\t\tmodel = model_fn().to(device)\n",
    "\n",
    "\t\t\t# Update only classifier layer for fine-tuning\n",
    "\t\t\tfor param in model.parameters():\n",
    "\t\t\t\tparam.requires_grad = False\n",
    "\n",
    "\t\t\t# Fine-tune the last layer\n",
    "\t\t\tif hasattr(model, 'fc'):  # For ResNet models\n",
    "\t\t\t\tmodel.fc = nn.Linear(model.fc.in_features, 200).to(device)\n",
    "\t\t\telif hasattr(model, 'classifier'):  # For models like VGG or AlexNet\n",
    "\t\t\t\tmodel.classifier[-1] = nn.Linear(model.classifier[-1].in_features, 200).to(device)\n",
    "\n",
    "\t\t\tfor param in model.fc.parameters():\n",
    "\t\t\t\tparam.requires_grad = True\n",
    "\n",
    "\t\t\t# Select optimizer based on config\n",
    "\t\t\tif config['optimizer'] == 'SGD':\n",
    "\t\t\t\toptimizer = optim.SGD(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\t\t\telif config['optimizer'] == 'AdamW':\n",
    "\t\t\t\toptimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "\t\t\tcriterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\t\t\tfor epoch in range(num_epochs):\n",
    "\t\t\t\tprint(f\"\tEpoch {epoch + 1}/{num_epochs}\")\n",
    "\t\t\t\ttrain_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\t\t\t\tval_loss, top1_error, top5_error = evaluate(model, val_loader, criterion, device)    \n",
    "\n",
    "\t\t\t\tresults.append({\n",
    "\t\t\t\t\t'model': model_name,\n",
    "\t\t\t\t\t'config': config_name,\n",
    "\t\t\t\t\t'epoch': epoch + 1,\n",
    "\t\t\t\t\t'train_loss': train_loss,\n",
    "\t\t\t\t\t'train_acc': train_acc,\n",
    "\t\t\t\t\t'val_loss': val_loss,\n",
    "\t\t\t\t\t'top1_error': top1_error,\n",
    "\t\t\t\t\t'top5_error': top5_error\n",
    "\t\t\t\t})\n",
    "\n",
    "\treturn results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "results = compare_models(\n",
    "\tmodels=model_definitions,\n",
    "\tmodel_configs=model_configs,\n",
    "\thyperparameters=hyperparameters,\n",
    "\tdata_dir=data_directory,\n",
    "\tnum_epochs=hyperparameters['epochs'],\n",
    "\tdevice=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Export results to CSV\n",
    "current_time = time.strftime(\"%Y%m%d-%H%M\")\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f\"./exports/data/regular_models_results_(many configurations)_{current_time}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
